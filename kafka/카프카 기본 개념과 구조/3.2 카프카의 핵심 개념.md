여러 우수한 기업들이 카프카를 서둘러 도입하고 많은 개발자가 좋아하는 이유는 앞서 살펴봤듯이 카프카의 높은 처리량,
빠른 응답 속도, 안정성 때문이다. 

하지만 왜 카프카가 그렇게 안정적인지 어떻게 높은 처리량을 갖게 됐는지에 관한 구체적인 내용은 그다지 알려지지 않았다.

이번 절에서는 카프카가 높은 처리량과 안정성을 지니게 된 특성들을 하나씩 살펴보며 카프카를 좀 더 깊이 파고들어 본다.

# 분산 시스템

---
분산 시스템은 네트워크상에서 연결된 컴퓨터들의 그룹을 말하며, 단일 시슽메이 갖지 못한 높은 성능을 목표로 한다.

이러한 분산 시스템은 성능이 높다는 장점 이외에도 하나의 서버 또는 노드 등에 장애가 발생할 때 
다른 서버 또는 노드가 대신 처리하므로 장애 대응이 탁월하며, 부하가 높은 경우에는 시스템 확장이 용이하다는 장점도 있다.

카프카도 분산 시스템이므로 최초 구성한 클러스터의 리소스가 한계치에 도달해 더욱 높은 메시지 처리량이 필요한 경우,
브로커를 추가하는 방식으로 확장이 가능하다.

# 페이지 캐시

---
카프카는 높은 처리량을 얻기 위해 몇 가지 기능을 추가했는데, 그중 대표적인 것이 바로 페이지 캐시(page cache)의 이용이다.
운영체제(OS)는 성능을 높이기 위해 꾸준히 진화하고 개선되고 있는데, 특히 페이지 캐시의 활용이 대표적이다.

카프카 역시 OS의 페이지 캐시를 활용하는 방식으로 설계되어 있다. 페이지 캐시는 직접 디스크에 읽고 쓰는 대신 
물리 메모리 중 애플리케이션이 사용하지 않는 일부 잔여 메모리를 활용한다.

카프카가 OS의 페이지 캐시를 이용한다는 것은 카프카가 직접 디스크에서 읽고 쓰기를 하지 않고 페이지 캐시를 통해 읽고 쓰기를 한다고 이해하면 된다.

# 배치 전송 처리

---
카프카는 프로듀서, 컨슈머 클라이언트들과 서로 통신하며, 이들 사이에서 수많은 메시지를 주고 받는다.
이때 발생하는 수많은 통신을 묶어서 처리할 수 있다면, 단건으로 통신할 때에 비해 네트워크 오버헤드를 줄일 수 있을 뿐만 아니라 장기적으로는 더욱 빠르고 효율적으로 처리할 수 있다.

# 압축 전송

---
카프카는 메시지 전송 시 좀 더 성능이 좋은 압축 전송을 사용한느 것을 권장한다. 카프카에서 지원하는 압축 타입은 다음과 같다.

* gzip
* snappy
* lz4
* zstd

압축만으로도 네트워크 대역폭이나 회선 비용을 줄일 수 있는데, 앞서 설명한 배치 전송과 결합해 사용한다면 더욱 높은 효과를 얻게 된다.
파일 하나를 압축하는 것보다 비슷한 파일 10개, 혹은 20개를 압축하는 쪽의 압축 효율이 더욱 좋기 때문이다.

일반적으로 높은 압축률이 필요한 경우라면 gzip이나 zstd를 권장하고, 빠른 응답 속도가 필요하다면 lz4나 snappy를 권장한다.
실제로 또 다른 결과가 나타날 수 있으니 메시지를 전송해보면서 압축 타입별로 직접 테스트를 해보는 것이 좋다.

# 토픽, 파티션, 오프셋

---
카프카는 토픽(topic)이라는 곳에 데이터를 저장한다. 토픽은 병렬 처리를 위해 여러 개의 파티션(partition)이라는 단위로 다시 나뉜다.
카프카에서는 이와 같은 파티셔닝을 통해 단 하나의 토픽이라도 높은 처리량을 수행할 수 있다.

이 파티션의 메시지가 저장되는 위치를 오프셋(offset)이라고 부르며, 오프셋은 순차적으로 증가하는 숫자 형태로 되어있다.

각 파티션에서의 오프셋은 고유한 숫자로, 카프카에서는 오프셋을 통해 메시지의 순서를 보장하고 컨슈머에서는 마지막까지 읽은 위치를 알 수도 있다.

# 고가용성 보장
카프카는 분산 시스템이기 때문에 하나의 서버나 노드가 다운되어도 다른 서버 또는 노드가 장애가 발생한 서버의 역할을 대신해 안정적인 서비스가 가능하다.

이러한 고가용성을 보장하기 위해 카프카에서는 리플리케이션 기능을 제공한다.
카프카에서 제공하는 리플리케이션 기능은 토픽 자체를 복제하는 것이 아니라 토픽의 파티션을 복제하는 것이다.

토픽을 생성할 때 옵션으로 리플리케이션 팩터 수를 지정할 수 있으며, 이 숫자에 따라 리플리케이션들이 존재하게 된다.
원본과 리플리케이션의 구분을 위해 흔히 마스터, 미러 같은 용어를 사용하는데 카프카에서는 리더(leader)와 팔로워(follower)라고 부른다.

일반적으로 팔로워 수가 많을수록 안정적이고 좋을 거라 생각할 수 있겠지만 팔로워의 수가 많다고 딱히 좋은 것은 아니다.
팔로워의 수만큼 결국 브로커의 디스크 공간도 소비되므로 이상적인 리플리케이션 팩터 수를 유지해야 한다.

> 일반적으로 카프카에서는 리플리케이션 팩터 수를 3으로 구성하도록 권장한다.

리더는 프로듀서, 컨슈머로부터 오는 모든 읽기와 쓰기 요청을 처리하며, 팔로워는 오직 리더로부터 리플리케이션 하게 된다.

# 주키퍼의 의존성
카프카를 언급하면서 빼놓을 수 없는 부분이 바로 주키퍼(zookeeper)이다. 앞에서도 이미 여러 번 간략히 언급한 바 있는 주키퍼는 하둡의 서브 프로젝트 중 하나로 출발해 2011년 아파치의 탑레벨 프로젝트로 승격됐다.

주키퍼는 여러 대의 서버를 앙상블(클러스터)로 구성하고, 살아있는 노드 수가 과반수 이상이 유지된다면 지속적인 서비스가 가능한 구조이다.
따라서 주키퍼는 반드시 홀수로 구성되어야 한다.

하지만 최근 들어 카프카가 점점 성장하면서 주키퍼 성능의 한계가 드러나기 시작했다.
주키퍼의 한계에서 벗어나고자 카프카에서 주키퍼에 대한 의존성을 제거하려는 움직임을 한참 진행 중이다.